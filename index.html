<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>New Frontiers Federated Learning 2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css?v=0.0" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo_mit_ibm.png" alt="" /></span> -->
						<h1>Closing the Gap between Academia and Industry in Federated Learning: Challenges on Privacy, Fairness, Robustness, Personalization and Data Ownership</h1>
						<p> Location: <a href="http://whereis.mit.edu/?go=E52"> </a>Virtual<br /> (<a href="#dir">Getting Here</a>)</p>
						<p> December 13-14, 2021 </p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Introduction</a></li>
							<li><a href="https://bayesian-methods.eventbrite.com">Registration</a></li>
							<li><a href="#cfp">Call For Participation</a></li>
							<li><a href="#sched">Schedule</a></li>
							<li><a href="#inv">Invited Speakers</a></li>
							<!-- <li><a href="#panel">Featured Panelists</a></li> -->
							<li><a href="#org">Organizers</a></li>
							<li><a href="https://ibm.biz/ai-research-week">IBM Research AI Week</a></li>
							<!-- <li><a href="https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=7848">Previous Summits</a></li> -->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>About</h2>
										</header>
										<!-- <p style="text-align:center"><img src="images/banner_1.jpg" alt="MIT Watson AI Lab, Cambridge, MA"
											width="1024" height="256">
										</p> -->
										<p style="text-align:justify"> On Friday September 20th, 2019 as part of <a href="https://ibm.biz/ai-research-week">IBM Research's
											AI week</a> we will be hosting the first workshop on Practical Bayesian methods for Big Data.
										 </p>

										<p  style="text-align:justify"> Bayesian methods have long benefited from their ability to both coherently represent uncertainty and incorporate prior knowledge, but have traditionally struggled to scale to both large data and large models. Deep learning approaches empirically demonstrate the benefits of learning large over-parameterized models from large data, but struggle with producing well calibrated uncertainties. Research attempting to both scale up Bayesian methods and combine the benefits of either paradigm has recently garnered significant attention. Examples include deep generative models and Bayesian neural networks. This workshop will advance and accelerate research on statistical underpinnings of methods at this intersection, including recent advances in Bayesian approaches for learning neural network based models , deep learning methods for Bayesian modeling, methods for scaling up Bayesian inference to large models and data, and use of classical statistical tools for measuring robustness and reliability of deep learning models.
										</p>




									</div>

							</section>

						<!-- First Section -->
							<section id="cfp" class="main special">
								<header class="major">
									<h2>Call For Participation</h2>
								</header>
								<p style="text-align:justify">
						 We invite researchers to submit work in (but not limited to) the following areas:
		<ul style="text-align:left">
		<li> Bayesian approaches for learning neural network based models. </li>
		<li> Advances in deep generative modeling. </li>
		<li> Deep learning methods for Bayesian modeling. </li>
		<li> Methods for scaling up Bayesian inference to large models and data. </li>
		<li> Methods for measuring robustness and reliability of statistical models. </li>
	</ul>
</P>

<h2 style="text-align:left">Submissions</h2>
<p style="text-align:justify">
	Submission can be made via an
	<a href=https://easychair.org/cfp/PBMB-2019>
		EasyChair submission.</a>
		The submission should be in the form of an extended abstract and should not exceed 3 pages (excluding references) in PDF format using NeurIPS style. Submissions of new ideas, recently published works and/or extension of existing works are welcome. Parallel submissions or submissions of under-review works are also permitted. Author names do not need to be anonymized.

		Submission will be accepted as contributed 15-minute talks or poster presentations. The final versions will be posted on the workshop website (and are archival but do not constitute a proceeding).Â 
	</p>

	<h2 style="text-align:left"> Key Dates </h2>
	<ul style="text-align:left">
		<li>Abstract and submission deadline: September 10, 2019. </li>
		<li>Meeting Date: September 20, 2019. </li>
	</ul>


	<h2 style="text-align:left">Attendance</h2>
	<p style="text-align:justify"> For each accepted paper or poster,
	at least one author must attend the workshop and present the
	paper/poster.
	<br>

	</section>

						<!-- Agenda -->
							<section id="sched" class="main special">
								<header class="major">
									<h2>Schedule</h2>
								</header>
								<table style="width:100%">
								<tr>
									<td>8:45-9:00 AM</td>	 <td> Registration and Check-in </br>
										 </td>
								</tr>
								<tr>
								<td>9:00-9:15 AM</td>	 <td>Welcome and Opening Remarks </br>
									</td>
								</tr>
								<tr>
									<td>9:15-10:00 AM</td>	 <td> Invited Talk: </br> Finale Doshi-Velez
									</br> <a href="#fdv">Models for Bayesian Neural Networks</a> </br>
										 </td>
								</tr>
							 	<tr><td>10:00-10:45 AM</td>	 <td> Invited Talk: </br> Natesh Pillai
									</br> <a href="#np">Accelerating MCMC algorithms in computer intensive models and applications to large data sets</a> </br> </td>
								</tr>
								<tr><td>10:45-11:00 AM</td>	 <td>Coffee Break </td>
								</tr>
								<tr><td>11:00-11:45 AM</td>  <td> Invited Talk:  </br>  Tamara Broderick </br> <a href="#tb">The Kernel Interaction Trick: Fast Bayesian Discovery of
									Pairwise Interactions in High Dimensions</a> </br> </td>
								</tr>
								<tr>
									<td>11:45-1:00 PM</td>	 <td> Lunch </br>
										 </td>
								</tr>
								<tr>
									<td>1:00-1:45 PM</td>	 <td> Invited Talk: </br> Jan-Willem van de Meent
										  </br> <a href="#jvdm">Integrating Deep Learning and Probabilistic Programming </br>
</a>
										 </td>
								</tr>
								<tr>
									<td>1:45-2:45 PM</td>	 <td> <a href="#spot">Contributed Talks</a> </br> 10 minutes each, plus 2-minute Q/A
										  </br>
										 </td>
								</tr>
								<tr>
									<td>2:45-3:00 PM</td>	 <td> Coffee Break </br>
										 </td>
								</tr>
								<tr>
									<td>3:00-3:45 PM</td>	 <td> Invited Talk: </br> Justin Solomon
									</br> <a href="#js">Approximating and Manipulating Probability Distributions with Transport</a> </br>
										 </td>
								</tr>
								<tr>
									<td>3:45-4:30 PM</td>	 <td> Invited Talk: </br> Vikash Mansinghka
									</br> <a href="#vm">Probabilistic Programming and Artificial Intelligence</a> </br>
										 </td>
								</tr>

							</table>

							</section>

							<!-- Invited Speakers -->
							<section id="inv" class="main special">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
							<ul class="features">
								<li>
									<span><img src="images/Tamara.jpg" height="192"/></span>
									<h3><a href=http://www.tamarabroderick.com>Tamara Broderick</a></h3>
									<p> Associate Professor, EECS, </br>
									MIT </p>
								</li>
								<li>
									<span><img src="images/F_DoshiVelez.jpg" height="192"/></span>
									<h3><a href=https://finale.seas.harvard.edu>Finale Doshi-Velez</a></h3>
									<p> Assistant Professor, Computer Science, </br>
									Harvard University</p>
								</li>
								<li>
									<span><img src="images/Natesh.jpg" height="192"/></span>
									<h3><a href=http://www.people.fas.harvard.edu/~pillai/Welcome.html>Natesh Pillai</a></h3>
									<p>Professor, Statistics, </br>
                                        Harvard University</p>
								</li>
								<li>
									<span><img src="images/Jan.jpg" height="192"/></span>
									<h3><a href=http://www.ccs.neu.edu/home/jwvdm/>Jan-Willem van de Meent</a></h3>
									<p>Assistant Professor, College of Computer and Information Science,</br>
										Northeastern</p>
								</li>
								<li>
									<span><img src="images/J_Solomon.jpg" height="192"/></span>
									<h3> <a href=https://people.csail.mit.edu/jsolomon/>Justin Solomon</a></h3>
									<br> Assistant Professor, EECS, </br>
									MIT </p>
								</li>
                                <li>
                                    <span><img src="images/V_Mansinghka.jpg" height="192"/></span>
                                    <h3> <a href="http://probcomp.csail.mit.edu/principal-investigator/">Vikash Mansinghka</a></h3>
                                    <br> Principal Research Scientist </br>
                                    MIT </p>
                                </li>
							</ul>
							</section>

							<!-- Organizers -->
							<section id="org" class="main special">
							<h2>Organizing Committee</h2>

							<table style="float:center">
							  <tr>
							    <td><img src="images/N_Hoang.png" height="192"/></td>
							    <td><img src="images/M_Yurokchin.png" height="192"/></td>
									<td><img src="images/K_Severson.png" height="192"/></td>
							  </tr>
							  <tr>
							    <td><a href="https://sites.google.com/site/idmhtn/home/publications">Nghia Hoang</a> </td>
							    <td><a href="http://www-personal.umich.edu/~moonfolk/">Mikhail Yurokchin</td>
									<td><a href="https://kseverso.github.io">Kristen Severson</a></td>
							  </tr>
								<tr>
									<td><img src="images/P_Sattigeri.png" height="192"/></td>
									<td><img src="images/A_Srivastava.png" height="192"/></td>
									<td><img src="images/S_Ghosh.png" height="192"/></td>
								</tr>
								<tr>
									<td><a href="">Prasanna Sattigeri</a></td>
									<td><a href="">Akash Srivastava</a></td>
									<td><a href="">Soumya Ghosh</a></td>
								</tr>
							</table>
							</section>

							<!-- Spotlights -->
							<section id="spot" class="main special">
								<header class="major">
									<h2>Contributed Talks</h2>
								</header>
								<table style="width:100%">
									<tr>
									<td>1:45-1:57 PM</td>	 <td> Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow </br>
										Tuan Anh Le
										</br>
										 </td>
								</tr>
									<tr>
									<td>1:57-2:09 PM</td>	 <td> Assessing the Robustness of Bayesian Dark Knowledge to Posterior Uncertainty </br>
										Meet Vadera
										</br>
										 </td>
								</tr>
									<tr>
									<td>2:09-2:21 PM</td>	 <td> Learning Free Energies with Bayesian Networks </br>
										Jonathan Vandermause
										</br>
										 </td>
								</tr>
									<tr>
									<td>2:21-2:33 PM</td>	 <td> Dual Neural Network Architecture for Determining Epistemic and Aleatoric Uncertainties </br>
										Ravinath Kausik
										</br>
										 </td>
								</tr>
									<tr>
									<td>2:33-2:45 PM</td>	 <td> Neural Tree Kernel Learning </br>
										  </br>
										 </td>
								</tr>
							</table>
							</section>
		<section id="inv" class="main special">
								<header class="major">
									<h2>Invited Talks</h2>
								</header>
								<h2 style="text-align:left" id="fdv">Models for Bayesian Neural Networks, Finale Doshi-Velez</h2>
								<p style="text-align:justify">Bayesian neural networks (BNN) are often described as a more practical, scalable alternative to other forms of distributions over functions (e.g. Gaussian processes).
However, "easy" or "obvious" ways of specifying priors don't necessarily result in the properties that we expect--or want.  In this talk, I will talk about some of the work in
our group toward creating BNN models with nicer properties, as well as incorporating expert knowledge.  Finally, I'll touch on some considerations for inference.  This is joint
work with: Soumya Ghosh, Jiayu Yao, Yaniv Yacoby, Weiwei Pan, Melanie Pradier, Wanqian Yang, Moritz Graule, Lars Lorch, Anirudh Suresh, Srivatsan Srinivasan, Stefan Depeweg, Miguel Hernandez-Lobato
								</p>

			<h2 style="text-align:left" id="np">Accelerating MCMC algorithms in computer intensive models and applications to large data sets, Natesh Pillai</h2>
								<p style="text-align:justify">We discuss a new framework for accelerating MCMC algorithms for sampling from posterior distributions in the context of computationally intensive models. We proceed by constructing
local surrogates of the forward model within the Metropolis-Hastings kernel, borrowing ideas from deterministic approximation theory, optimization, and experimental design. Our work
departs from previous work in surrogate-based inference by exploiting useful convergence characteristics of local approximations. We prove the ergodicity of our approximate Markov chain
and show that it samples asymptotically from the exact posterior distribution of interest. We describe variations of the algorithm that construct either local polynomial approximations
or Gaussian process regressors, thus spanning two important classes of surrogate models. Our theoretical results reinforce the key observation underlying this paper: when the likelihood
has some local regularity, the number of model evaluations per MCMC step can be greatly reduced, without incurring significant bias in the Monte Carlo average. Our numerical experiments
demonstrate order-of-magnitude reductions in the number of forward model evaluations used in representative ODE or PDE inference problems, in both real and synthetic data examples. We will
also give applications of our theory for problems involving intractable likelihoods and large data sets. Joint work with Andrew Davis, Patrick Conrad, Youssef Marzouk, Aaron Smith.
								</p>
			<h2 style="text-align:left" id="tb"> The Kernel Interaction Trick: Fast Bayesian Discovery of
Pairwise Interactions in High Dimensions, Tamara Broderick</h2>
			<p style="text-align:justify">Discovering interaction effects on a response of interest is
a fundamental problem faced in biology, medicine, economics, and many
other scientific disciplines. In theory, Bayesian methods for
discovering pairwise interactions enjoy many benefits such as coherent
uncertainty quantification, the ability to incorporate background
knowledge, and desirable shrinkage properties. In practice, however,
Bayesian methods are often computationally intractable for even
moderate-dimensional problems. Our key insight is that many
hierarchical models of practical interest admit a particular Gaussian
process (GP) representation; the GP allows us to capture the posterior
with a vector of O(p) kernel hyper-parameters rather than O(p^2)
interactions and main effects. With the implicit representation, we
can run Markov chain Monte Carlo (MCMC) over model hyper-parameters in
time and memory linear in p per iteration. We focus on
sparsity-inducing models and show on datasets with a variety of
covariate behaviors that our method: (1) reduces runtime by orders of
magnitude over naive applications of MCMC, (2) provides lower Type I
and Type II error relative to state-of-the-art LASSO-based approaches,
and (3) offers improved computational scaling in high dimensions
relative to existing Bayesian and LASSO-based approaches.</p>
			<h2 style="text-align:left" id="jvdm">Integrating Deep Learning and Probabilistic Programming, Jan-Willem Van de Meent</h2>
			<p style="text-align:justify">A clear lesson from ongoing advances in deep learning is that large overparameterized models can achieve unsurpassed performance in settings where a sufficiently large amount
of data and computation are available. A much more open question is how we can improve the generalization properties of these models when we have a limited amount of data,
or a limited amount of labels. In this talk I will discuss how we can combine the principles of probabilistic programming with those of deep learning to design models that
incorporate inductive biases that aid generalization. I will provide examples of models that can be trained in an unsupervised or semi-supervised manner to learn structured
representations of interpretable variables of interest. I will also discuss ongoing research to improve scalability of inference and evaluate generalization properties of learned models.</p>
			<h2 style="text-align:left" id="js">Approximating and Manipulating Probability Distributions with Transport, Justin Solomon</h2>
								<p style="text-align:justify">The theory of optimal transport defines a metric on the space of probability distributions that lifts the metric of the underlying geometric domain.  The structure of this geometry on the space of probability distributions has several favorable properties for tasks in inference and learning.  In this talk, I will  introduce recent work applying transport to a variety of computational tasks in learning and statistics, including computation of coresets for efficient and approximate learning, distributionally-robust learning in the semi-supervised setting, and overcoming symmetry issues in Bayesian inference.  I also will discuss computational techniques employed to overcome the computational cost of evaluating and manipulating transport distances in practice.  [Joint work with E. Chien, S. Claici, C. Frogner, F. Mirzazadeh, P. Monteiller, and M. Yurochkin]
								</p>
			<h2 style="text-align:left" id="vm">Probabilistic Programming and Artificial Intelligence, Vikash Mansingka</h2>
			<p style="text-align:justify"> Probabilistic programming is an emerging field at the intersection of programming languages, probability theory, and artificial intelligence. This talk will show how to use recently
developed probabilistic programming languages to build systems for robust 3D computer vision, without requiring any labeled training data; for automatic modeling of complex real-world time series;
and for machine-assisted analysis of experimental data in synthetic biology that is too small and messy for standard approaches from machine learning and statistics.
This talk will use these applications to illustrate recent technical innovations in probabilistic programming that formalize and unify modeling approaches from multiple eras of AI, including
generative models, neural networks, symbolic programs, causal Bayesian networks, and hierarchical Bayesian modeling. Specifically, it will present languages in which models are represented using
executable code, and in which inference is programmable using novel constructs for Monte Carlo, optimization-based, and neural inference. It will also present techniques for Bayesian learning of
probabilistic program structure and parameters from real-world data. Finally, this talk will review challenges and research opportunities in the development and use of general-purpose probabilistic
programming languages that performant enough and flexible enough for real-world AI engineering.
			</p>

							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section id="dir">
							<h2>Location</h2>
							<p style="text-align:justify">MIT Samberg Center, Cambridge, MA, United States.</br>
							<i>Samberg center is located in Kendall square and easily accessible by public transportaion.
						  It is a short walk from the <a href=https://www.google.com/maps/dir/Kendall,+Cambridge,+MA+02142/MIT+Samberg+Conference+Center,+Memorial+Drive,+Cambridge,+MA/@42.3614109,-71.0867415,17z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89e370af454cadd1:0xed3f24407286838c!2m2!1d-71.0855752!2d42.3624823!1m5!1m1!1s0x89e370a679984489:0x5c8d65db5c0d7efe!2m2!1d-71.0836961!2d42.360732!3e2>Kendall/MIT</a> stop on the red line and from <a href="https://www.google.com/maps/dir/Lechmere,+Cambridge,+MA/MIT+Samberg+Conference+Center,+Memorial+Drive,+Cambridge,+MA/@42.3652177,-71.0851221,16z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89e370be837d0fa7:0x1c5777c77406d5f9!2m2!1d-71.077113!2d42.370088!1m5!1m1!1s0x89e370a679984489:0x5c8d65db5c0d7efe!2m2!1d-71.0836961!2d42.360732!3e2">Lechmere</a> on the green line.
							We highly encourage using public transportation to get here. </i></p>


						</section>
						<section>
							<h2>Contact</h2>
								<dd>Nghia Hoang, nghiaht at ibm dot com </dd>
						<p class="copyright">&copy; Workshop Organizing Committee. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
