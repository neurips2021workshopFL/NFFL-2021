<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>New Frontier in Federated Learning 2021</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css?v=0.0" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/logo_mit_ibm.png" alt="" /></span> -->
						<h1>Closing the Gap between Academia and Industry in Federated Learning: Challenges on Privacy, Fairness, Robustness, Personalization and Data Ownership</h1>
						<p> Location: <a href="http://whereis.mit.edu/?go=E52"> </a>Virtual<br /> (<a href="#dir">Getting Here</a>)</p>
						<p> December 13-14, 2021 </p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#intro" class="active">Introduction</a></li>
							<!-- <li><a href="https://bayesian-methods.eventbrite.com">Registration</a></li> -->
							<li><a href="#cfp">Call For Participation</a></li>
							<li><a href="#sched">Schedule</a></li>
							<li><a href="#inv">Invited Speakers</a></li>
							<!-- <li><a href="#panel">Featured Panelists</a></li> -->
							<li><a href="#org">Committee</a></li>
							<li><a href="#papers">Accepted Papers</a></li>
							<!-- <li><a href="https://ibm.biz/ai-research-week">IBM Research AI Week</a></li> -->
							<!-- <li><a href="https://researcher.watson.ibm.com/researcher/view_group_subpage.php?id=7848">Previous Summits</a></li> -->
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
						<!-- Introduction -->
							<section id="intro" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>About</h2>
										</header>
										<!-- <p style="text-align:center"><img src="images/banner_1.jpg" alt="MIT Watson AI Lab, Cambridge, MA"
											width="1024" height="256">
										</p> -->
										<p style="text-align:justify"> Federated Learning (FL) has recently emerged as the de facto framework for distributed machine learning (ML) that preserves the privacy of data, especially in the proliferation of mobile and edge devices with their increasing capacity for storage and computation. To fully utilize the vast amount of geographically distributed, diverse and privately owned data that is stored across these devices, FL provides a platform on which local devices can build their own local models whose training processes can be synchronized via sharing differential parameter updates. This was done without exposing their private training data, which helps mitigate the risk of privacy violation, in light of recent policies such as the General Data Protection Regulation (GDPR). Such potential use of FL has since then led to an explosive attention from the ML community, resulting in a vast, growing amount of both theoretical and empirical literature that push FL closer to being the new standard of ML as a democratized data analytic service.
										 </p>

										<p  style="text-align:justify"> Interestingly, as FL comes closer to being deployable in real-world scenarios, it also surfaces a growing set of challenges on trustworthiness, fairness, auditability, scalability, robustness, security, privacy preservation, decentralizability, data ownership and personalizability that are all becoming increasingly important in many interrelated aspects of our digitized society. Such challenges are particularly important in economic landscapes that do not have the presence of big tech corporations with big data and are instead driven by government agencies and institutions with valuable data locked up or small-to-medium enterprises & start-ups with limited data and little funding. With this forethought, the workshop envisions the establishment of an AI ecosystem that facilitates data and model sharing between data curators as well as interested parties in the data and models while protecting personal data ownership. 
										</p>
										
										<p  style="text-align:justify"> This raises the following questions: </br>
										1. Data curators may own different types of ML models. Due to their own interest in protecting the IP, there is no reason to believe that they would be willing to share information on their model architectures or parameters. Thus, if we are to facilitate meaningful collaboration in such cases, how then do data curators aggregate and distill latent knowledge from their heterogeneous, black-box models to bring home the distilled model(s) for future use? </br>
										
										2. How do we incentivize data curators to come together to share their data for model building? How does a participant know that the other data curator(s) are contributing valuable, authentic and safe data to the collaboration (and vice versa)? How do we incentivize them to collaborate? In this view, there is a need to consider data auditability and fairness in data sharing based on their respective contributions. Furthermore, as far as personal data ownership goes, how do we guarantee the right to be forgotten in terms of the participantâ€™s data footprint? 
										</p>
										
										<p  style="text-align:justify"> We believe addressing these challenges will make another key milestone in shaping FL as a democratized machine learning (ML) service supported by a trustworthy AI ecosystem built on the aforementioned concepts.
										</p>
										
									</div>

							</section>

						<!-- First Section -->
							<section id="cfp" class="main special">
								<header class="major">
									<h2>Call For Participation</h2>
								</header>
								<p style="text-align:justify">
						 We invite researchers to submit work in (but not limited to) the following areas:
		<ul style="text-align:left">
			<li>Personalized Federated Learning and/or Meta Learning.</li>
			<li>Differential Privacy in Federated Learning.</li>
			<li>Fairness in Federated Learning.</li>
			<li>Optimization for Large-Scale Federated Learning Systems.</li>
			<li>Certifiable Robustness for Federated Learning.</li>
			<li>Trustworthiness, Auditability and Verification in Federated Learning.</li>
			<li>Model Aggregation and Protecting Personal Data Ownership.</li>
	</ul>
</P>

<h2 style="text-align:left">Submissions</h2>
<p style="text-align:justify">
	TBD
	</p>

	<h2 style="text-align:left"> Key Dates </h2>
	<ul style="text-align:left">
		<li>Abstract and submission deadline: TBD </li>
		<li>Meeting Date: TBD </li>
	</ul>


	<h2 style="text-align:left">Attendance</h2>
	<p style="text-align:justify"> For each accepted paper or poster,
	at least one author must attend the workshop and present the
	paper/poster.
	<br>

	</section>

						<!-- Agenda -->
							<section id="sched" class="main special">
								<header class="major">
									<h2>Schedule (EST Time)</h2>
								</header>
								<table style="width:100%">
								<tr>
									<td> <strong>Session 1</strong> </td> <td> </td>  </br>
								</tr>
								<tr>
									<td>8:30 AM - 9:00 AM</td>	 <td> Pre-workshop networking on Slack/Zoom </br>
										 </td>
								</tr>
								<tr>
									<td>9:00 AM - 9:15 AM</td>	 <td> Opening Remarks </br>
										 </td>
								</tr>
								<tr>
								<td>9:15 AM - 10:00 AM</td>	 <td>Invited Talk 1 </br>
									</td>
								</tr>
								<tr>
								<td>10:00 AM - 10:15 AM</td>	 <td>Contributed Talk 1 </br>
									</td>
								</tr>
								<tr>
								<td>10:15 AM - 10:30 AM</td>	 <td>Contributed Talk 2 </br>
									</td>
								</tr>
								<tr>
								<td>10:30 AM - 10:45 AM</td>	 <td>Contributed Talk 3 </br>
									</td>
								</tr>
								<tr>
								<td>10:45 AM - 11:30 AM</td>	 <td>Invited Talk 2 </br>
									</td>
								</tr>
								<tr>
									<td> <strong>Session 2</strong> </td> <td> </td> </br>
								</tr>
								<tr>
									<td>11:30 AM - 1:00 PM</td>	 <td> Virtual lunch break with opportunities </br> for socialization on Slack/Zoom </br>
										 </td>
								</tr>
								<tr>
								<td>1:00 PM - 1:45 PM</td>	 <td>Invited Talk 3 </br>
									</td>
								</tr>
								<tr>
								<td>1:45 PM - 2:00 PM</td>	 <td>Contributed Talk 4 </br>
									</td>
								</tr>
								<tr>
								<td>2:00 PM - 2:15 PM</td>	 <td>Contributed Talk 5 </br>
									</td>
								</tr>
								<tr>
								<td>2:15 PM - 2:30 PM</td>	 <td>Contributed Talk 6 </br>
									</td>
								</tr>
								<tr>
									<td> <strong>Session 3</strong> </td> <td> </td> </br>
								</tr>
								<tr>
									<td>2:30 PM - 3:30 PM</td>	 <td> Virtual lunch break with opportunities </br> for socialization on Slack/Zoom </br>
										 </td>
								</tr>
								<tr>
								<td>3:30 PM - 4:15 PM</td>	 <td>Invited Talk 4 </br>
									</td>
								</tr>
								<tr>
								<td>4:15 PM - 5:00 PM</td>	 <td>Invited Talk 5 </br>
									</td>
								</tr>
								<tr>
								<td>5:00 PM - 5:50 PM</td>	 <td>Panel Discussion -- can also be replaced or shorten to fit in </br>
									</td>
								</tr>
								<tr>
								<td>5:50 PM - 6:00 PM</td>	 <td>Closing Remarks </br>
									</td>
								</tr>
								<tr>
								<td>6:00 PM - 7:00 PM</td>	 <td>Post-workshop networking on Slack/Zoom </br>
									</td>
								</tr>
								</table>

							</section>

							<!-- Invited Speakers -->
							<section id="inv" class="main special">
								<header class="major">
									<h2>Invited Speakers</h2>
								</header>
							<ul class="features">
								<li>
									<span><img src="images/Tamara.jpg" height="192"/></span>
									<h3><a href=http://www.tamarabroderick.com>Tamara Broderick</a></h3>
									<p> Associate Professor, EECS, </br>
									MIT </p>
								</li>
								<li>
									<span><img src="images/F_DoshiVelez.jpg" height="192"/></span>
									<h3><a href=https://finale.seas.harvard.edu>Finale Doshi-Velez</a></h3>
									<p> Assistant Professor, Computer Science, </br>
									Harvard University</p>
								</li>
								<li>
									<span><img src="images/Natesh.jpg" height="192"/></span>
									<h3><a href=http://www.people.fas.harvard.edu/~pillai/Welcome.html>Natesh Pillai</a></h3>
									<p>Professor, Statistics, </br>
                                        Harvard University</p>
								</li>
								<li>
									<span><img src="images/Jan.jpg" height="192"/></span>
									<h3><a href=http://www.ccs.neu.edu/home/jwvdm/>Jan-Willem van de Meent</a></h3>
									<p>Assistant Professor, College of Computer and Information Science,</br>
										Northeastern</p>
								</li>
								<li>
									<span><img src="images/J_Solomon.jpg" height="192"/></span>
									<h3> <a href=https://people.csail.mit.edu/jsolomon/>Justin Solomon</a></h3>
									<br> Assistant Professor, EECS, </br>
									MIT </p>
								</li>
                                <li>
                                    <span><img src="images/V_Mansinghka.jpg" height="192"/></span>
                                    <h3> <a href="http://probcomp.csail.mit.edu/principal-investigator/">Vikash Mansinghka</a></h3>
                                    <br> Principal Research Scientist </br>
                                    MIT </p>
                                </li>
							</ul>
							</section>

							<!-- Organizers -->
							<section id="org" class="main special">
							<h2>Organizing Committee</h2>

							<table style="float:center">
							  <tr>
							    <td><img src="images/N_Hoang.png" height="192"/></td>
							    <td><img src="images/M_Yurokchin.png" height="192"/></td>
									<td><img src="images/K_Severson.png" height="192"/></td>
							  </tr>
							  <tr>
							    <td><a href="https://sites.google.com/site/idmhtn/home/publications">Nghia Hoang</a> </td>
							    <td><a href="http://www-personal.umich.edu/~moonfolk/">Mikhail Yurokchin</td>
									<td><a href="https://kseverso.github.io">Kristen Severson</a></td>
							  </tr>
								<tr>
									<td><img src="images/P_Sattigeri.png" height="192"/></td>
									<td><img src="images/A_Srivastava.png" height="192"/></td>
									<td><img src="images/S_Ghosh.png" height="192"/></td>
								</tr>
								<tr>
									<td><a href="">Prasanna Sattigeri</a></td>
									<td><a href="">Akash Srivastava</a></td>
									<td><a href="">Soumya Ghosh</a></td>
								</tr>
							</table>
							</section>

							<!-- Spotlights -->
							<section id="spot" class="main special">
								<header class="major">
									<h2>Contributed Talks</h2>
								</header>
								<table style="width:100%">
									<tr>
									<td>1:45-1:57 PM</td>	 <td> Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow </br>
										Tuan Anh Le
										</br>
										 </td>
								</tr>
									<tr>
									<td>1:57-2:09 PM</td>	 <td> Assessing the Robustness of Bayesian Dark Knowledge to Posterior Uncertainty </br>
										Meet Vadera
										</br>
										 </td>
								</tr>
									<tr>
									<td>2:09-2:21 PM</td>	 <td> Learning Free Energies with Bayesian Networks </br>
										Jonathan Vandermause
										</br>
										 </td>
								</tr>
									<tr>
									<td>2:21-2:33 PM</td>	 <td> Dual Neural Network Architecture for Determining Epistemic and Aleatoric Uncertainties </br>
										Ravinath Kausik
										</br>
										 </td>
								</tr>
									<tr>
									<td>2:33-2:45 PM</td>	 <td> Neural Tree Kernel Learning </br>
										  </br>
										 </td>
								</tr>
							</table>
							</section>
		<section id="inv" class="main special">
								<header class="major">
									<h2>Invited Talks</h2>
								</header>
								<h2 style="text-align:left" id="fdv">Models for Bayesian Neural Networks, Finale Doshi-Velez</h2>
								<p style="text-align:justify">Bayesian neural networks (BNN) are often described as a more practical, scalable alternative to other forms of distributions over functions (e.g. Gaussian processes).
However, "easy" or "obvious" ways of specifying priors don't necessarily result in the properties that we expect--or want.  In this talk, I will talk about some of the work in
our group toward creating BNN models with nicer properties, as well as incorporating expert knowledge.  Finally, I'll touch on some considerations for inference.  This is joint
work with: Soumya Ghosh, Jiayu Yao, Yaniv Yacoby, Weiwei Pan, Melanie Pradier, Wanqian Yang, Moritz Graule, Lars Lorch, Anirudh Suresh, Srivatsan Srinivasan, Stefan Depeweg, Miguel Hernandez-Lobato
								</p>

			<h2 style="text-align:left" id="np">Accelerating MCMC algorithms in computer intensive models and applications to large data sets, Natesh Pillai</h2>
								<p style="text-align:justify">We discuss a new framework for accelerating MCMC algorithms for sampling from posterior distributions in the context of computationally intensive models. We proceed by constructing
local surrogates of the forward model within the Metropolis-Hastings kernel, borrowing ideas from deterministic approximation theory, optimization, and experimental design. Our work
departs from previous work in surrogate-based inference by exploiting useful convergence characteristics of local approximations. We prove the ergodicity of our approximate Markov chain
and show that it samples asymptotically from the exact posterior distribution of interest. We describe variations of the algorithm that construct either local polynomial approximations
or Gaussian process regressors, thus spanning two important classes of surrogate models. Our theoretical results reinforce the key observation underlying this paper: when the likelihood
has some local regularity, the number of model evaluations per MCMC step can be greatly reduced, without incurring significant bias in the Monte Carlo average. Our numerical experiments
demonstrate order-of-magnitude reductions in the number of forward model evaluations used in representative ODE or PDE inference problems, in both real and synthetic data examples. We will
also give applications of our theory for problems involving intractable likelihoods and large data sets. Joint work with Andrew Davis, Patrick Conrad, Youssef Marzouk, Aaron Smith.
								</p>
			<h2 style="text-align:left" id="tb"> The Kernel Interaction Trick: Fast Bayesian Discovery of
Pairwise Interactions in High Dimensions, Tamara Broderick</h2>
			<p style="text-align:justify">Discovering interaction effects on a response of interest is
a fundamental problem faced in biology, medicine, economics, and many
other scientific disciplines. In theory, Bayesian methods for
discovering pairwise interactions enjoy many benefits such as coherent
uncertainty quantification, the ability to incorporate background
knowledge, and desirable shrinkage properties. In practice, however,
Bayesian methods are often computationally intractable for even
moderate-dimensional problems. Our key insight is that many
hierarchical models of practical interest admit a particular Gaussian
process (GP) representation; the GP allows us to capture the posterior
with a vector of O(p) kernel hyper-parameters rather than O(p^2)
interactions and main effects. With the implicit representation, we
can run Markov chain Monte Carlo (MCMC) over model hyper-parameters in
time and memory linear in p per iteration. We focus on
sparsity-inducing models and show on datasets with a variety of
covariate behaviors that our method: (1) reduces runtime by orders of
magnitude over naive applications of MCMC, (2) provides lower Type I
and Type II error relative to state-of-the-art LASSO-based approaches,
and (3) offers improved computational scaling in high dimensions
relative to existing Bayesian and LASSO-based approaches.</p>
			<h2 style="text-align:left" id="jvdm">Integrating Deep Learning and Probabilistic Programming, Jan-Willem Van de Meent</h2>
			<p style="text-align:justify">A clear lesson from ongoing advances in deep learning is that large overparameterized models can achieve unsurpassed performance in settings where a sufficiently large amount
of data and computation are available. A much more open question is how we can improve the generalization properties of these models when we have a limited amount of data,
or a limited amount of labels. In this talk I will discuss how we can combine the principles of probabilistic programming with those of deep learning to design models that
incorporate inductive biases that aid generalization. I will provide examples of models that can be trained in an unsupervised or semi-supervised manner to learn structured
representations of interpretable variables of interest. I will also discuss ongoing research to improve scalability of inference and evaluate generalization properties of learned models.</p>
			<h2 style="text-align:left" id="js">Approximating and Manipulating Probability Distributions with Transport, Justin Solomon</h2>
								<p style="text-align:justify">The theory of optimal transport defines a metric on the space of probability distributions that lifts the metric of the underlying geometric domain.  The structure of this geometry on the space of probability distributions has several favorable properties for tasks in inference and learning.  In this talk, I will  introduce recent work applying transport to a variety of computational tasks in learning and statistics, including computation of coresets for efficient and approximate learning, distributionally-robust learning in the semi-supervised setting, and overcoming symmetry issues in Bayesian inference.  I also will discuss computational techniques employed to overcome the computational cost of evaluating and manipulating transport distances in practice.  [Joint work with E. Chien, S. Claici, C. Frogner, F. Mirzazadeh, P. Monteiller, and M. Yurochkin]
								</p>
			<h2 style="text-align:left" id="vm">Probabilistic Programming and Artificial Intelligence, Vikash Mansingka</h2>
			<p style="text-align:justify"> Probabilistic programming is an emerging field at the intersection of programming languages, probability theory, and artificial intelligence. This talk will show how to use recently
developed probabilistic programming languages to build systems for robust 3D computer vision, without requiring any labeled training data; for automatic modeling of complex real-world time series;
and for machine-assisted analysis of experimental data in synthetic biology that is too small and messy for standard approaches from machine learning and statistics.
This talk will use these applications to illustrate recent technical innovations in probabilistic programming that formalize and unify modeling approaches from multiple eras of AI, including
generative models, neural networks, symbolic programs, causal Bayesian networks, and hierarchical Bayesian modeling. Specifically, it will present languages in which models are represented using
executable code, and in which inference is programmable using novel constructs for Monte Carlo, optimization-based, and neural inference. It will also present techniques for Bayesian learning of
probabilistic program structure and parameters from real-world data. Finally, this talk will review challenges and research opportunities in the development and use of general-purpose probabilistic
programming languages that performant enough and flexible enough for real-world AI engineering.
			</p>

							</section>
					</div>

				<!-- Footer -->
					<footer id="footer">
						<section id="dir">
							<h2>Location</h2>
							<p style="text-align:justify">MIT Samberg Center, Cambridge, MA, United States.</br>
							<i>Samberg center is located in Kendall square and easily accessible by public transportaion.
						  It is a short walk from the <a href=https://www.google.com/maps/dir/Kendall,+Cambridge,+MA+02142/MIT+Samberg+Conference+Center,+Memorial+Drive,+Cambridge,+MA/@42.3614109,-71.0867415,17z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89e370af454cadd1:0xed3f24407286838c!2m2!1d-71.0855752!2d42.3624823!1m5!1m1!1s0x89e370a679984489:0x5c8d65db5c0d7efe!2m2!1d-71.0836961!2d42.360732!3e2>Kendall/MIT</a> stop on the red line and from <a href="https://www.google.com/maps/dir/Lechmere,+Cambridge,+MA/MIT+Samberg+Conference+Center,+Memorial+Drive,+Cambridge,+MA/@42.3652177,-71.0851221,16z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x89e370be837d0fa7:0x1c5777c77406d5f9!2m2!1d-71.077113!2d42.370088!1m5!1m1!1s0x89e370a679984489:0x5c8d65db5c0d7efe!2m2!1d-71.0836961!2d42.360732!3e2">Lechmere</a> on the green line.
							We highly encourage using public transportation to get here. </i></p>


						</section>
						<section>
							<h2>Contact</h2>
								<dd>Nghia Hoang, nghiaht at ibm dot com </dd>
						<p class="copyright">&copy; Workshop Organizing Committee. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

	</body>
</html>
